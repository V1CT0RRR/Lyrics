# SIFRank for keyword extraction

SIFRank is an unsupervised keyword extraction algorithm that relies on text embedding using pretrained model from larger corpus. The key idea is to compare the embedding of each candidate key phrase with the embedding of the whole document to find the most similar keywords that best summarize the document. 

## Environment
```
python==3.6
scikit-learn==0.22.2
allennlp==0.8.4
overrides==3.1.0
```


## Usage

```
python SIFRank/sifrank.py -h
usage: sifrank.py [-h] [-k K] [-path PATH] [-document DOCUMENT] [-doc_seg]
                  [-emb_align] [-sifrank_plus] [-use_spacy]

optional arguments:
  -h, --help          show this help message and exit
  -k K                number of keyword candidates to extract. default to 10
  -path PATH          path to the lyrics csv file, default to none
  -document DOCUMENT  document to extract keywords from. default to none
  -doc_seg            use document segmentation to speed up embedding. default
                      to true
  -emb_align          embedding alignment. default to true
  -sifrank_plus       use sifrank+ with position score. default to true
  -use_spacy          use spacy language model for preprocessing. default to false

python SIFRank/sifrank.py
```

ELMO weights: ```elmo_2x4096_512_2048cnn_2xhighway_options.json``` and ```elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5``` can be downloaded [here](https://allenai.org/allennlp/software/elmo) and save to ```models/elmo/``` directory.

## Implementation

### Preprocessing
We perform the standard preprocessing procedure on the document with nltk, including character cleaning, tokenizing, pos-tagging, lemmatizing and noun-phrase chunk extraction using fixed grammar. Note that sentence splitting is not necessary for keyword extraction as the embedding of the whole document is computed for comparison.

The ```-use_spacy``` option enables preprocessing using the spaCy library for tokenizing, pos-tagging and noun-phrase chunk extraction with a pretrained pipeline. From our experiment, spaCy preprocessing gives better result for TextRank on the Inspec dataset but not for SIFRank. Therefore this option is by default turned off.

### Word Embedding
We used pretrained ELMo provided by [Allennlp](https://allenai.org/allennlp/software/elmo) for word embedding. For each input document with T tokens, an embedding of size $num\_batch \times 3 \times T \times 1024$ is generated by ELMo, where each of the 3 layers represents character, grammar and context-dependent semantic information for every token.

### Document and Candidate Embeddings
For the downstream task, we initialized word weights with a vocabulary extracted from large corpus, in this case, the English wikipedia: ```models/enwiki_vocab_min200.txt```. Each token in the embeddings obtained in the previous step is weighted accordingly and taken average to become the document vector.

For noun-phrase candidates extracted from the preprocessing step, their embeddings are indexed from the previous document embedding, and similarly, computed weighted average to obtain a vector that represents the phrase itself. The document embedding and each candidate embeddings are compared for similarity for a final score of how representative a candidate phrase is to the document. The top-k candidate phrases that are the most similar are saved as results.

### Tweaks
#### Document Segmentation
To speed up the embedding computation of ELMo, documents can be segmented into batches of sentences with a minimum number (in this implementation 16) tokens. Batched embeddings are later merged and reshaped.

#### Embedding Alignment
To avoid loss of the complete context of the document after segmentation, an embedded anchor for each word is defined as the average of all embeddings of this word in the segmented document embedding and later used to replace all occureences of this word in the embedding. 

#### SIFRank+: Position-based Weight for Long Documents
Under the assumption that authors tend to write key topic in the beginning of documents, SIFRank+ added a position-based weight for candidate key phrases. The position score is computed as $\tilde{p}(NP) = softmax(\frac{1}{p_1 + \mu})$, where $p_1$ is the index of the first occurence of a phrase among all candidates, and $\mu$ is for position weight tuning. Position weights are then applied to similarities of candidate phrases to the document to obtain the final similarity.

## Evaluation
Evaluation is done on the [Inspec](https://huggingface.co/datasets/midas/inspec) dataset with 2000 (1000 training, 500 test and 500 valid) abstracts from scientific papers and key phrases annotated by professional indexers.

|    |   precision |   recall |   accuracy | document_segmentation   | embedding_alignment   | SIFRank+   | spacy_preprocessing   |
|---:|------------:|---------:|-----------:|:------------------------|:----------------------|:-----------|:----------------------|
|  0 |    0.255726 | 0.24178  |   **0.248558** | True                    | True                  | False      | False                 |
|  1 |    0.249373 | 0.235773 |   0.242382 | True                    | True                  | True       | False                 |
|  2 |    0.19142  | 0.18092  |   0.186022 | True                    | True                  | True       | True                  |
|  3 |    0.168088 | 0.158868 |   0.163348 | True                    | True                  | False      | True                  |

## Example

> Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.

An example of SIFRank extracting key phrases from the above text from the TextRank paper. 

```
0.929145: linear constraints
0.913474: systems
0.889705: linear Diophantine equations
0.869187: Compatibility
0.846884: algorithms
0.845055: corresponding algorithms
0.829582: compatibility
0.827061: system
0.823212: components
0.822642: minimal supporting set
```